{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## open file"
      ],
      "metadata": {
        "id": "_1ydor0KALvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[kaggle data link](https://www.kaggle.com/competitions/spaceship-titanic)"
      ],
      "metadata": {
        "id": "L6-ZjCfuwEiv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LGh1h7KUB0Hd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxNmBlp2SeXk",
        "outputId": "f6bb642f-866a-4637-96f0-94c7685df232"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FB9G2JmlB0Hg"
      },
      "outputs": [],
      "source": [
        "folder_path = '/content/drive/MyDrive/'\n",
        "data_path = os.path.join(folder_path,'kaggle','spaceship-titanic','data')\n",
        "train_path = os.path.join(data_path,'finish_train.csv')\n",
        "test_path = os.path.join(data_path,'finish_test.csv')\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## models"
      ],
      "metadata": {
        "id": "PS6IO4tw28LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process, model_selection\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split, ShuffleSplit, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from tqdm import tqdm\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb"
      ],
      "metadata": {
        "id": "fL8sICdVC8MI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TErdoHIXB0Hn"
      },
      "source": [
        "## train model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train/test split   = 8 : 2"
      ],
      "metadata": {
        "id": "txY1xbh1TRYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train[train.columns.difference(['Transported'])].values\n",
        "y_train = train['Transported'].astype(int).values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2)"
      ],
      "metadata": {
        "id": "LUoMLmvt_sFc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ltPvBhuPB0Ho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "587c124a-e541-4f1b-b026-5138ab00ecda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3510, number of negative: 3444\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001382 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2168\n",
            "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 27\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.504745 -> initscore=0.018982\n",
            "[LightGBM] [Info] Start training from score 0.018982\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.64      0.73       871\n",
            "           1       0.71      0.90      0.79       868\n",
            "\n",
            "    accuracy                           0.77      1739\n",
            "   macro avg       0.79      0.77      0.76      1739\n",
            "weighted avg       0.79      0.77      0.76      1739\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.16      0.27       871\n",
            "           1       0.54      0.98      0.70       868\n",
            "\n",
            "    accuracy                           0.57      1739\n",
            "   macro avg       0.72      0.57      0.48      1739\n",
            "weighted avg       0.72      0.57      0.48      1739\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.81      0.80       871\n",
            "           1       0.81      0.79      0.80       868\n",
            "\n",
            "    accuracy                           0.80      1739\n",
            "   macro avg       0.80      0.80      0.80      1739\n",
            "weighted avg       0.80      0.80      0.80      1739\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.78      0.79       871\n",
            "           1       0.78      0.81      0.79       868\n",
            "\n",
            "    accuracy                           0.79      1739\n",
            "   macro avg       0.79      0.79      0.79      1739\n",
            "weighted avg       0.79      0.79      0.79      1739\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.79      0.81       871\n",
            "           1       0.80      0.84      0.82       868\n",
            "\n",
            "    accuracy                           0.81      1739\n",
            "   macro avg       0.81      0.81      0.81      1739\n",
            "weighted avg       0.81      0.81      0.81      1739\n",
            "\n"
          ]
        }
      ],
      "source": [
        "logit_model = linear_model.LogisticRegressionCV()\n",
        "sgd_model = linear_model.SGDClassifier()\n",
        "rf_model = RandomForestClassifier()\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "lgbm_model = lgb.LGBMClassifier()\n",
        "\n",
        "logit_model.fit(X_train, y_train)\n",
        "sgd_model.fit(X_train, y_train)\n",
        "rf_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logit_model.predict(X_test)\n",
        "print(classification_report(y_test,y_pred))\n",
        "y_pred = sgd_model.predict(X_test)\n",
        "print(classification_report(y_test,y_pred))\n",
        "y_pred = rf_model.predict(X_test)\n",
        "print(classification_report(y_test,y_pred))\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "print(classification_report(y_test,y_pred))\n",
        "y_pred = lgbm_model.predict(X_test)\n",
        "print(classification_report(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logit_model = linear_model.LogisticRegressionCV()\n",
        "sgd_model = linear_model.SGDClassifier()\n",
        "rf_model = RandomForestClassifier()\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "lgbm_model = lgb.LGBMClassifier()\n",
        "\n",
        "models_vote = VotingClassifier(\n",
        "        estimators=[('logit',logit_model),('sgd',sgd_model),('rf',rf_model),('xgb',xgb_model),('lgbm',lgbm_model)],\n",
        "        voting='hard')\n",
        "models_vote.fit(X_train, y_train)\n",
        "y_pred = models_vote.predict(X_test)\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "models_vote = VotingClassifier(\n",
        "        estimators=[('logit',logit_model),('rf',rf_model),('xgb',xgb_model),('lgbm',lgbm_model)], # sgd 沒辦法算機率\n",
        "        voting='soft')\n",
        "models_vote.fit(X_train, y_train)\n",
        "y_pred = models_vote.predict(X_test)\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "ogsty2M_Nc1o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac8bc845-4414-4cd1-df45-c3764365c77e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3510, number of negative: 3444\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001287 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2168\n",
            "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 27\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.504745 -> initscore=0.018982\n",
            "[LightGBM] [Info] Start training from score 0.018982\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.75      0.79       871\n",
            "           1       0.77      0.85      0.81       868\n",
            "\n",
            "    accuracy                           0.80      1739\n",
            "   macro avg       0.80      0.80      0.80      1739\n",
            "weighted avg       0.80      0.80      0.80      1739\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 3510, number of negative: 3444\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001460 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2168\n",
            "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 27\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.504745 -> initscore=0.018982\n",
            "[LightGBM] [Info] Start training from score 0.018982\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.76      0.79       871\n",
            "           1       0.78      0.83      0.80       868\n",
            "\n",
            "    accuracy                           0.79      1739\n",
            "   macro avg       0.80      0.79      0.79      1739\n",
            "weighted avg       0.80      0.79      0.79      1739\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test model"
      ],
      "metadata": {
        "id": "LIemB6zizpxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train[train.columns.difference(['Transported'])].values\n",
        "y_train = train['Transported'].astype(int).values\n",
        "X_test = test[test.columns.difference(['Transported'])].values"
      ],
      "metadata": {
        "id": "d-56cWaRNTzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## top5 model"
      ],
      "metadata": {
        "id": "clzGr8oadfXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLA_top5 = [\n",
        "    linear_model.LogisticRegressionCV(),\n",
        "    linear_model.SGDClassifier(),\n",
        "    ensemble.RandomForestClassifier(),\n",
        "    xgb.XGBClassifier(),\n",
        "    lgb.LGBMClassifier()\n",
        "    ]\n",
        "\n",
        "grid_param = [\n",
        "        [{\n",
        "      #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
        "      'fit_intercept': [True, False], #default: True\n",
        "      'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n",
        "      'random_state': [0]\n",
        "        }],\n",
        "        [{\n",
        "      #SGDC;assifier\n",
        "      'loss':['hinge', 'log_loss', 'modified_huber'],\n",
        "      'random_state': [0]\n",
        "        }],\n",
        "        [{\n",
        "      #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
        "      'n_estimators': [10, 50, 100, 300], #default=10\n",
        "      'criterion': ['gini', 'entropy'], #default=”gini”\n",
        "      'max_depth': [2, 4, 6, 8, 10, None], #default=None\n",
        "      'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n",
        "      'random_state': [0]\n",
        "        }],\n",
        "        [{\n",
        "      #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n",
        "      'learning_rate': [.01, .03, .05, .1, .25], #default: .3\n",
        "      'max_depth': [1,2,4,6,8,10], #default 2\n",
        "      'n_estimators': [10, 50, 100, 300],\n",
        "      'seed': [0]\n",
        "        }],\n",
        "\n",
        "        [{}]\n",
        "        ]"
      ],
      "metadata": {
        "id": "w_eT-ZAyqjkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_split = model_selection.ShuffleSplit(n_splits = 5, test_size = 0.2, random_state = 0)\n",
        "\n",
        "best_param_list = []\n",
        "for clf, param in zip (MLA_top5, grid_param):\n",
        "  MLA_name = clf.__class__.__name__\n",
        "  best_search = model_selection.GridSearchCV(estimator = clf, param_grid = param, cv = cv_split, scoring = 'roc_auc')\n",
        "  best_search.fit(X_train, y_train)\n",
        "  best_param = best_search.best_params_\n",
        "  print(MLA_name)\n",
        "  print(best_param)\n",
        "  best_param_list.append(best_param)"
      ],
      "metadata": {
        "id": "FEGV7lpwfgTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.mkdir('answer')"
      ],
      "metadata": {
        "id": "f_QQ-LV-Zxrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "for model, params in zip(MLA_top5, best_param_list):\n",
        "  model.set_params(**params)\n",
        "  models.append(model)"
      ],
      "metadata": {
        "id": "qw7Xv2VKcrCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in models:\n",
        "  model_name = model.__class__.__name__\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  answer = pd.concat([test['PassengerId'], pd.Series(y_pred, name='Transported')], axis=1)\n",
        "  answer.replace({0: False, 1: True}, inplace=True)\n",
        "  answer.to_csv('answer/'+model_name+'.csv', index=False)\n",
        "  print(model_name)"
      ],
      "metadata": {
        "id": "dw_ZLT5F13XA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e6d97f-26e2-48a4-8af9-d3993effd04b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegressionCV\n",
            "SGDClassifier\n",
            "RandomForestClassifier\n",
            "XGBClassifier\n",
            "[LightGBM] [Info] Number of positive: 4378, number of negative: 4315\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001323 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2186\n",
            "[LightGBM] [Info] Number of data points in the train set: 8693, number of used features: 27\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503624 -> initscore=0.014495\n",
            "[LightGBM] [Info] Start training from score 0.014495\n",
            "LGBMClassifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_vote = VotingClassifier(\n",
        "        estimators=[('logit',models[0]),('sdg',models[1]),('rfm',models[2]),('xgb',models[3]),('lgbm',models[4])],\n",
        "        voting='hard')\n",
        "models_vote.fit(X_train, y_train)\n",
        "y_pred = models_vote.predict(X_test)\n",
        "answer = pd.concat([test['PassengerId'], pd.Series(y_pred, name='Transported')], axis=1)\n",
        "answer.replace({0: False, 1: True}, inplace=True)\n",
        "answer.to_csv('model/models_hard_vote.csv', index=False)\n",
        "\n",
        "models_vote = VotingClassifier(\n",
        "        estimators=[('logit',models[0]),('rfm',models[2]),('xgb',models[3]),('lgbm',models[4])],  # sgd 沒辦法算機率\n",
        "        voting='soft')\n",
        "models_vote.fit(X_train, y_train)\n",
        "y_pred = models_vote.predict(X_test)\n",
        "answer = pd.concat([test['PassengerId'], pd.Series(y_pred, name='Transported')], axis=1)\n",
        "answer.replace({0: False, 1: True}, inplace=True)\n",
        "answer.to_csv('model/models_soft_vote.csv', index=False)"
      ],
      "metadata": {
        "id": "W521sFic5buo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54fcd992-e50c-4016-efd9-b339414efa48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 4378, number of negative: 4315\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001343 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2186\n",
            "[LightGBM] [Info] Number of data points in the train set: 8693, number of used features: 27\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503624 -> initscore=0.014495\n",
            "[LightGBM] [Info] Start training from score 0.014495\n",
            "[LightGBM] [Info] Number of positive: 4378, number of negative: 4315\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001439 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2186\n",
            "[LightGBM] [Info] Number of data points in the train set: 8693, number of used features: 27\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503624 -> initscore=0.014495\n",
            "[LightGBM] [Info] Start training from score 0.014495\n"
          ]
        }
      ]
    }
  ]
}